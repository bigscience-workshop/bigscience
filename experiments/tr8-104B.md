# Train 8 104B wide tune up

note: this tune up table is somewhat invalid since during the tune up a mistake was made in  `FFN_HIDDEN_SIZE` which was incorrectly set to a much lower value, so the tests below were really tested a 58B model. So the TFLOPs numbers in this section are incorrect. but I'm not sure how to fix it, since I don't think the formula applies when the model is lopsided. The numbers in sections afterwards are correct.

```
NLAYERS=32
NHIDDEN=16384
NHEADS=32
SEQ_LEN=2048
VOCAB_SIZE=50257
```

BS=1024, SIZE=104B,

| NNODES |  TP |  PP |  DP | MBS | Speed | TFlops | Notes                 |
| -----: | --: | --: | --: | --: | ----: | -----: | --------------------: |
|     32 |   4 |  32 |   1 |   1 |   256 |   27.2 | 31.5GB                |
|     64 |   4 |  64 |   1 |   1 |   155 |   22.5 | 24GB                  |
|        |     |     |     |     |       |        |                       |

```
perl -le '$ng=32*4; $sp=256; $ms=104; $gbs=1048; print $ms*4*2*1024*$gbs / ( $sp * $ng * 1e3)'
perl -le '$ng=64*4; $sp=155; $ms=104; $gbs=1048; print $ms*4*2*1024*$gbs / ( $sp * $ng * 1e3)'
```

(ng = total gpus, ms = model size in B, gbs = global batch size, sp = throughput in seconds)

BS=2048


| NNODES |  TP |  PP |  DP | MBS | Speed | TFlops | Notes                 |
|  ----: | --: | --: | --: | --: | ----: | -----: | --------------------: |
|     32 |   4 |  32 |   1 |   1 |   586 |  23.26 | GB                    |
|     64 |   4 |  64 |   1 |   1 |   301 |  22.64 | 25GB                  |
|        |     |     |     |     |       |        |                       |


```
perl -le '$ng=32*4; $sp=586; $ms=104; $gbs=2048; print $ms*4*2*1024*$gbs / ( $sp * $ng * 1e3)'
perl -le '$ng=64*4; $sp=301; $ms=104; $gbs=2048; print $ms*4*2*1024*$gbs / ( $sp * $ng * 1e3)'
```



e.g. interactive tuning on 32 nodes

```
salloc --account=six@gpu --constraint=v100-32g --nodes=32 --ntasks=32 --cpus-per-task=40 --gres=gpu:4 --hint=nomultithread --time=3:00:00 bash --rcfile $six_ALL_CCFRWORK/start-prod
```




## BNB

w/ `--use-bnb-optimizer`

| NNODES |  TP |  PP |  DP | MBS | Speed | TFlops | Notes                           |
|  ----: | --: | --: | --: | --: | ----: | -----: | --------------------:           |
|     32 |   4 |  16 |   2 |   1 |   681 |   40.0 | 31GB                            |
|     32 |   2 |  32 |   2 |   1 |   633 |   43.0 | 31GB                            |
|     32 |   1 |  64 |   2 |   1 |       |        | 32GB OOMs                       |
|     32 |   4 |  32 |   1 |   1 |   688 |   39.6 | 27GB (same conf as normal 104B) |
|        |     |     |     |     |       |        |                                 |

```
perl -le '$ng=32*4; $sp=633; $ms=104; $gbs=2048; $seqlen=2048; print $ms*4*2*$seqlen*$gbs / ( $sp * $ng * 1e3)'
```

To ensure we are comparing apples to apples, trying to using the same allocations re-testing the baseline (but I'm not I get the same nodes all the time).

The baseline of 104B experiment w/o `--use-bnb-optimizer` that we have been using for all experiments

using the `main` branch:

| NNODES |  TP |  PP |  DP | MBS | Speed | TFlops | Notes                           |
|  ----: | --: | --: | --: | --: | ----: | -----: | --------------------:           |
|     32 |   4 |  32 |   1 |   1 | 696   |  39.17 | 30GB (same conf as normal 104B) |
|        |     |     |     |     |       |        |                                 |

using the old `big-science` branch

| NNODES |  TP |  PP |  DP | MBS | Speed | TFlops | Notes                           |
|  ----: | --: | --: | --: | --: | ----: | -----: | --------------------:           |
|     32 |   4 |  32 |   1 |   1 | 706   | 38.6   | 30GB (same conf as normal 104B) |
|        |     |     |     |     |       |        |                                 |
